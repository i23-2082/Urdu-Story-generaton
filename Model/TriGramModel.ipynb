{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Add Tokenizer path to sys.path\n",
    "sys.path.append(\"../Tokenizer\")\n",
    "from bpe_tokenizer import BPETokenizer\n",
    "\n",
    "# ==========================================================\n",
    "# CONSTANTS\n",
    "# ==========================================================\n",
    "START = \"<START>\"\n",
    "EOT = \"\\uFFF2\"   # Must match preprocessing special token\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD TRAINED TOKENIZER\n",
    "# ==========================================================\n",
    "# Using the fixed BPETokenizer class\n",
    "tokenizer = BPETokenizer.load(\"../Tokenizer/bpe_tokenizer.pkl\")\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))\n",
    "\n",
    "# Ensure START and EOT tokens exist in vocab\n",
    "if START not in tokenizer.vocab:\n",
    "    new_id = max(tokenizer.vocab.values() or [-1]) + 1\n",
    "    tokenizer.vocab[START] = new_id\n",
    "    tokenizer.id_to_token[new_id] = START\n",
    "\n",
    "if EOT not in tokenizer.vocab:\n",
    "    new_id = max(tokenizer.vocab.values() or [-1]) + 1\n",
    "    tokenizer.vocab[EOT] = new_id\n",
    "    tokenizer.id_to_token[new_id] = EOT\n",
    "\n",
    "# ==========================================================\n",
    "# TRIGRAM LANGUAGE MODEL\n",
    "# ==========================================================\n",
    "class TrigramLanguageModel:\n",
    "    def __init__(self, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
    "        # Check if lambdas sum to 1\n",
    "        total = lambda1 + lambda2 + lambda3\n",
    "        self.lambda1 = lambda1 / total\n",
    "        self.lambda2 = lambda2 / total\n",
    "        self.lambda3 = lambda3 / total\n",
    "\n",
    "        self.unigram = Counter()\n",
    "        self.bigram = Counter()\n",
    "        self.trigram = Counter()\n",
    "        \n",
    "        # For better performance and zero-handling\n",
    "        self.bigram_totals = Counter()  # N(w1, w2, *)\n",
    "        self.unigram_totals = Counter() # N(w1, *)\n",
    "        \n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, corpus, start_id):\n",
    "        print(\"Training model...\")\n",
    "        for tokens in corpus:\n",
    "            # Pad with START tokens\n",
    "            # For trigram, we need two START tokens to predict the first real token\n",
    "            full_tokens = [start_id, start_id] + tokens\n",
    "            self.total_tokens += len(full_tokens)\n",
    "\n",
    "            for i in range(len(full_tokens)):\n",
    "                w3 = full_tokens[i]\n",
    "                self.unigram[w3] += 1\n",
    "                self.vocab.add(w3)\n",
    "\n",
    "                if i >= 1:\n",
    "                    w2 = full_tokens[i-1]\n",
    "                    self.bigram[(w2, w3)] += 1\n",
    "                    self.unigram_totals[w2] += 1\n",
    "\n",
    "                if i >= 2:\n",
    "                    w1, w2 = full_tokens[i-2], full_tokens[i-1]\n",
    "                    self.trigram[(w1, w2, w3)] += 1\n",
    "                    self.bigram_totals[(w1, w2)] += 1\n",
    "\n",
    "        print(\"Training completed.\")\n",
    "        print(\"Model vocabulary size:\", len(self.vocab))\n",
    "        print(\"Total tokens processed:\", self.total_tokens)\n",
    "\n",
    "    def unigram_prob(self, w):\n",
    "        return self.unigram[w] / self.total_tokens if self.total_tokens else 0\n",
    "\n",
    "    def bigram_prob(self, w1, w2):\n",
    "        denom = self.unigram_totals[w1]\n",
    "        return self.bigram[(w1, w2)] / denom if denom else 0\n",
    "\n",
    "    def trigram_prob(self, w1, w2, w3):\n",
    "        denom = self.bigram_totals[(w1, w2)]\n",
    "        return self.trigram[(w1, w2, w3)] / denom if denom else 0\n",
    "\n",
    "    def interpolated_prob(self, w1, w2, w3):\n",
    "        return (\n",
    "            self.lambda1 * self.unigram_prob(w3) +\n",
    "            self.lambda2 * self.bigram_prob(w2, w3) +\n",
    "            self.lambda3 * self.trigram_prob(w1, w2, w3)\n",
    "        )\n",
    "\n",
    "    def generate(self, tokenizer, max_length=150):\n",
    "        start_id = tokenizer.vocab[START]\n",
    "        eot_id = tokenizer.vocab[EOT]\n",
    "\n",
    "        # Initial state\n",
    "        result_tokens = [start_id, start_id]\n",
    "        \n",
    "        # Precompute possible next tokens for efficiency\n",
    "        vocab_list = list(self.vocab)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            w1, w2 = result_tokens[-2], result_tokens[-1]\n",
    "            \n",
    "            # Calculate probabilities for all tokens in vocab\n",
    "            probs = []\n",
    "            for w3 in vocab_list:\n",
    "                probs.append(self.interpolated_prob(w1, w2, w3))\n",
    "            \n",
    "            # Normalize (should already be close to 1 but for safety)\n",
    "            total_p = sum(probs)\n",
    "            if total_p == 0:\n",
    "                break\n",
    "            probs = [p / total_p for p in probs]\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = random.choices(vocab_list, weights=probs, k=1)[0]\n",
    "            result_tokens.append(next_token)\n",
    "\n",
    "            if next_token == eot_id:\n",
    "                break\n",
    "\n",
    "        # Decode excluding START tokens\n",
    "        text = tokenizer.decode(result_tokens[2:])\n",
    "        return text.replace(EOT, \"\").strip()\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# ==========================================================\n",
    "try:\n",
    "    df = pd.read_csv(\"../PreProcessing/urdu_stories_processed.csv\", encoding=\"utf-8\")\n",
    "    stories = df[\"content\"].dropna().astype(str).tolist()\n",
    "    print(\"Stories loaded:\", len(stories))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Preprocessed data file not found.\")\n",
    "    stories = []\n",
    "\n",
    "# ==========================================================\n",
    "# ENCODE CORPUS\n",
    "# ==========================================================\n",
    "print(\"Encoding corpus...\")\n",
    "tokenized_corpus = []\n",
    "for story in stories:\n",
    "    encoded = tokenizer.encode(story)\n",
    "    # Append EOT if not present\n",
    "    if encoded and encoded[-1] != tokenizer.vocab[EOT]:\n",
    "        encoded.append(tokenizer.vocab[EOT])\n",
    "    if encoded:\n",
    "        tokenized_corpus.append(encoded)\n",
    "\n",
    "print(\"Stories encoded:\", len(tokenized_corpus))\n",
    "\n",
    "# ==========================================================\n",
    "# TRAIN TRIGRAM MODEL\n",
    "# ==========================================================\n",
    "model = TrigramLanguageModel(lambda1=0.05, lambda2=0.15, lambda3=0.8)\n",
    "if tokenized_corpus:\n",
    "    model.train(tokenized_corpus, tokenizer.vocab[START])\n",
    "\n",
    "# ==========================================================\n",
    "# SAVE MODEL\n",
    "# ==========================================================\n",
    "with open(\"trigram_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Trigram model saved successfully âœ…\")\n",
    "\n",
    "# ==========================================================\n",
    "# GENERATE SAMPLE STORY\n",
    "# ==========================================================\n",
    "if tokenized_corpus:\n",
    "    print(\"\\nGenerated Story:\\n\")\n",
    "    sample_story = model.generate(tokenizer)\n",
    "    print(sample_story)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
