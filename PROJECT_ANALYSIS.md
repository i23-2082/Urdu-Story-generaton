# NLP Project - Comprehensive Analysis

**Date:** February 20, 2026  
**Project:** Urdu Moral Stories Text Generation with Trigram Language Model  
**Status:** Core pipeline complete âœ…

---

## ğŸ“ PROJECT STRUCTURE

```
nlp/
â”œâ”€â”€ PreProcessing/          # Data preprocessing pipeline
â”‚   â”œâ”€â”€ pre_processing.ipynb
â”‚   â”œâ”€â”€ all_urdu_moral_stories.csv              (Original: 109 rows, 5 cols, 0.7 MB)
â”‚   â”œâ”€â”€ all_urdu_moral_stories_preprocessed.csv (Cleaned: 109 rows, 6 cols, 1.4 MB)
â”‚   â”œâ”€â”€ all_urdu_moral_stories_cleaned.csv      (Final: 109 rows, 1 col, 0.7 MB)
â”‚   â””â”€â”€ all_urdu_moral_stories_with_tokens.csv  (Tokenized: 109 rows, 1 col, 0.7 MB)
â”‚
â”œâ”€â”€ scaper/                 # Data collection & conversion
â”‚   â”œâ”€â”€ stories_scraper.py  (Web scraping from urdupoint.com)
â”‚   â”œâ”€â”€ pdf_scraper.py      (PDF extraction)
â”‚   â”œâ”€â”€ funny_stories_scraper.py
â”‚   â”œâ”€â”€ converter.py        (JSON â†’ CSV/XLSX conversion)
â”‚   â”œâ”€â”€ all_urdu_moral_stories.csv
â”‚   â”œâ”€â”€ all_urdu_moral_stories.json
â”‚   â””â”€â”€ all_urdu_moral_stories.xlsx
â”‚
â”œâ”€â”€ Tokenizer/              # Byte Pair Encoding (BPE) tokenizer
â”‚   â”œâ”€â”€ bpe_tokenizer.py    (BPE implementation - 122 lines)
â”‚   â”œâ”€â”€ toknizer.ipynb      (Training notebook)
â”‚   â”œâ”€â”€ bpe_tokenizer.pkl   (Trained tokenizer - 4.6 KB)
â”‚   â”‚   â””â”€â”€ vocab: 250 tokens
â”‚   â”‚   â””â”€â”€ merges: 166 merge operations
â”‚
â”œâ”€â”€ Model/                  # Trigram Language Model
â”‚   â”œâ”€â”€ TriGramModel.ipynb  (Training & generation)
â”‚   â””â”€â”€ trigram_model.pkl   (Trained model - 0.9 MB)
â”‚
â”œâ”€â”€ frontend/               (React app - not yet integrated)
â””â”€â”€ services/               (Planned microservices)
```

---

## ğŸ”„ DATA PIPELINE

### Phase 1: Data Collection âœ…
- **Source:** Urdu Point (urdupoint.com) - Moral Stories
- **Collection Method:** Selenium web scraping
- **Records:** 109 stories
- **Fields:** title, subtitle, date, content, url

### Phase 2: Data Preprocessing âœ…
**File:** `PreProcessing/pre_processing.ipynb`

**Steps Applied:**
1. **Remove English Characters** - Keep only Urdu text
   - Removes all a-z, A-Z
   - Preserves Urdu script and digits
   
2. **Normalize Unicode** - NFC normalization
   - Ensures consistent character representation
   - Handles combining characters

3. **Standardize Punctuation**
   - Normalize dashes (â€“, â€”) â†’ (-) 
   - Collapse multiple spaces
   - Remove duplicate Urdu punctuation

4. **Define Special Tokens**
   - `<EOS>` (U+FFF0): End of Sentence
   - `<EOP>` (U+FFF1): End of Paragraph
   - `<EOT>` (U+FFF2): End of Text/Story

**Output:** `all_urdu_moral_stories_with_tokens.csv`

### Phase 3: Tokenization (BPE) âœ…
**File:** `Tokenizer/bpe_tokenizer.py` (122 lines)

**Implementation:**
- Algorithm: Byte Pair Encoding (BPE)
- Vocabulary Size: 250 tokens
- Merge Operations: 166
- File Size: 4.6 KB

**Process:**
1. Build character-level vocabulary from training corpus
2. Count all adjacent character/token pairs
3. Merge most frequent pair repeatedly until vocab size = 250
4. Store merge operations for encoding/decoding

**Capabilities:**
```python
encode(text) â†’ list of token IDs
decode(token_ids) â†’ text
```

---

## ğŸ§  TRIGRAM LANGUAGE MODEL

**File:** `Model/TriGramModel.ipynb`

### Architecture

#### 1. Model Definition
```python
class TrigramLanguageModel:
    - lambda1 = 0.1  (unigram weight)
    - lambda2 = 0.3  (bigram weight)
    - lambda3 = 0.6  (trigram weight)
```

#### 2. Probability Calculation (MLE)

**Unigram:** P(w) = Count(w) / Total Tokens
**Bigram:** P(w_i | w_{i-1}) = Count(w_{i-1}, w_i) / Count(w_{i-1})
**Trigram:** P(w_i | w_{i-2}, w_{i-1}) = Count(w_{i-2}, w_{i-1}, w_i) / Count(w_{i-2}, w_{i-1})

#### 3. Interpolation (Smoothing)
```
P(w_i | w_{i-2}, w_{i-1}) = Î»â‚Â·P_uni + Î»â‚‚Â·P_bi + Î»â‚ƒÂ·P_tri
                          = 0.1Â·P_uni + 0.3Â·P_bi + 0.6Â·P_tri
```

**Purpose:** Avoids zero probabilities by backing off to lower-order models

### Training Results âœ…

| Metric | Value |
|--------|-------|
| Total Tokens | 85,270 |
| Vocabulary Size | 10,896 |
| Unique Unigrams | 10,896 |
| Unique Bigrams | 50,486 |
| **Unique Trigrams** | **76,128** |
| Model Size | 0.9 MB |

### Text Generation Features

1. **Prefix Support**
   - Accepts user-provided starting tokens
   - Auto-pads if < 2 tokens provided

2. **Temperature Control**
   - T=0.5: More deterministic (follows patterns)
   - T=1.0: Balanced
   - T=1.5: More creative/random

3. **Stopping Conditions**
   - Stops at `<EOT>` token
   - Respects max_length parameter
   - Handles unseen contexts gracefully

4. **Sampling Strategy**
   - Temperature-scaled probability redistribution
   - Multinomial sampling
   - Probabilistic beam search capable

---

## ğŸ“Š KEY STATISTICS

### Data Volume
- **Total Stories:** 109
- **Total Tokens:** 85,270
- **Average Story Length:** ~782 tokens
- **Unique Words (after tokenization):** 10,896

### Model Complexity
- **N-gram Coverage:**
  - Trigrams cover: 76,128 / 10,896^3 = 0.006% of possible space
  - Good coverage for frequent patterns
  
- **Sparsity:**
  - Interpolation weights specifically chosen to handle sparse data
  - Î»â‚ƒ=0.6 (trigram) gives highest weight where data exists
  - Î»â‚=0.1 (unigram) acts as fallback for unseen trigrams

---

## âš ï¸ CODE ISSUES & CLEANUP NEEDED

### In `TriGramModel.ipynb` (Cell 2)
**Problem:** Different interpolation weights than documentation
```python
# Current (in notebook)
lambda1 = 0.1  # unigram
lambda2 = 0.3  # bigram  
lambda3 = 0.6  # trigram

# Original (documented in model)
lambda1 = 0.5  # trigram
lambda2 = 0.3  # bigram
lambda3 = 0.2  # unigram
```
**Issue:** Inconsistent naming - unclear which is which

### Duplicate Code Issues (FIXED)
- âœ… Removed 4 duplicate/old cells
- âœ… Removed old generation cell with errors
- âœ… Reordered cells for correct execution

### Missing Docstrings
- `BPETokenizer.encode()` - should document expected input format
- `TrigramLanguageModel.generate()` - needs tokenizer interface docs

### Unused Import in `bpe_tokenizer.py`
```python
from collections import defaultdict, Counter
# Only Counter is used; defaultdict is unused
```

---

## ğŸš€ EXECUTION PIPELINE

### Order of Execution:
1. **Data Collection** â†’ `scaper/stories_scraper.py`
   - Output: `all_urdu_moral_stories.json`

2. **Conversion** â†’ `scaper/converter.py`
   - Input: JSON
   - Output: CSV, XLSX

3. **Preprocessing** â†’ `PreProcessing/pre_processing.ipynb`
   - Input: CSV
   - Steps: Remove English, Normalize Unicode, Standardize Punctuation, Add Special Tokens
   - Output: `all_urdu_moral_stories_with_tokens.csv`

4. **Tokenization** â†’ `Tokenizer/toknizer.ipynb`
   - Input: Preprocessed stories
   - Algorithm: BPE (250 vocab)
   - Output: `bpe_tokenizer.pkl`

5. **Model Training** â†’ `Model/TriGramModel.ipynb`
   - Input: Tokenized stories
   - Algorithm: Trigram with interpolation
   - Output: `trigram_model.pkl`

6. **Inference** â†’ Ready for deployment
   - Text generation from prefix
   - Temperature-controlled sampling

---

## ğŸ’¾ FILE SIZE OPTIMIZATION

| File | Current | Potential | Issue |
|------|---------|-----------|-------|
| trigram_model.pkl | 0.9 MB | 0.5 MB | Dictionary overhead |
| bpe_tokenizer.pkl | 4.6 KB | 2 KB | Can compress merges dict |
| all_urdu_moral_stories_with_tokens.csv | 0.7 MB | 0.4 MB | Redundant rows |
| **Total Data** | **~4.5 MB** | **~2.5 MB** | Compression possible |

---

## ğŸ” RECOMMENDATIONS

### Code Quality
1. **Fix lambda weight naming**
   - Rename for clarity: `lambda_uni`, `lambda_bi`, `lambda_tri`
   - Update all docstrings

2. **Remove unused imports**
   - Remove `defaultdict` from `bpe_tokenizer.py`

3. **Add comprehensive docstrings**
   - Include parameter types and return types
   - Add examples for tokenizer usage

4. **Add error handling**
   - Handle empty corpus in training
   - Validate token indices in decode()
   - Check for zero probabilities

### Performance
1. **Consider sparse tensor representation** for trigram_probs
2. **Cache interpolation probabilities** for frequent contexts
3. **Implement batching** for multi-story generation

### Features
1. **Beam search** instead of greedy sampling
2. **Top-K filtering** before sampling
3. **Nucleus sampling** (top-p) for better quality
4. **Perplexity evaluation** on test set

### Testing
1. Generate unit tests for tokenizer encode/decode
2. Test edge cases (unknown tokens, empty input, etc.)
3. Validate probability distributions sum to 1
4. Compare model output quality with baseline

---

## ğŸ“ SUMMARY

âœ… **Complete:** Data collection, preprocessing, tokenization, model training
âš ï¸ **Needs Review:** Code consistency, error handling, documentation
ğŸ”¨ **Next Steps:** Integration, testing, deployment

**Total Project Size:** ~4.5 MB  
**Model Performance:** Ready for inference with 85K+ token vocabulary
**Data Coverage:** 109 stories with comprehensive preprocessing pipeline

