{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb210986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T19:31:02.717852Z",
     "iopub.status.busy": "2026-02-18T19:31:02.716033Z",
     "iopub.status.idle": "2026-02-18T19:31:12.302324Z",
     "shell.execute_reply": "2026-02-18T19:31:12.301074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded: 394211 characters\n",
      "First 200 chars: ارسلان خان شیر جنگل میں آرام کر رہا تھا کہ اس نے بلی کو دیکھا تو اسے آواز دے کر بلایا۔￰شیر نے بلی سے پوچھا ”تم دیکھنے میں تو مجھ جیسی ہو لیکن تمہارا قد اتنا چھوٹا کیوں ہے؟￰“ بلی نے اداسی سے کہا، ”ظالم\n",
      "Initial vocab size: 84\n",
      "Goal: 166 merges to reach vocab size 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: Vocab size = 134, Merged ('د', 'ی') -> دی\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Vocab size = 184, Merged ('ٹ', 'ھ') -> ٹھ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150: Vocab size = 234, Merged ('پ', 'و') -> پو\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BPE Training Complete!\n",
      "Final vocab size: 250\n",
      "Tokenizer saved to bpe_tokenizer.pkl\n",
      "\n",
      "Sample vocabulary (first 20 and last 10 tokens):\n",
      "  0: '\u0004'\n",
      "  1: ' '\n",
      "  2: '!'\n",
      "  3: '\"'\n",
      "  4: '#'\n",
      "  5: '''\n",
      "  6: '('\n",
      "  7: ')'\n",
      "  8: '-'\n",
      "  9: '.'\n",
      "  10: '0'\n",
      "  11: '1'\n",
      "  12: '2'\n",
      "  13: '5'\n",
      "  14: '8'\n",
      "  15: ':'\n",
      "  16: '¿'\n",
      "  17: '،'\n",
      "  18: '؛'\n",
      "  19: '؟'\n",
      "  ...\n",
      "  240: 'گھر'\n",
      "  241: 'صاح'\n",
      "  242: 'ست'\n",
      "  243: 'ہاں'\n",
      "  244: 'قی'\n",
      "  245: 'ٹی'\n",
      "  246: 'ڑھ'\n",
      "  247: 'دے'\n",
      "  248: 'ساتھ'\n",
      "  249: 'رہے'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "\n",
    "# Load training data\n",
    "df = pd.read_csv('../PreProcessing/all_urdu_moral_stories_with_tokens.csv')\n",
    "training_text = ' '.join(df['content'].astype(str))\n",
    "\n",
    "print(f\"Training data loaded: {len(training_text)} characters\")\n",
    "print(f\"First 200 chars: {training_text[:200]}\")\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=250):\n",
    "        \"\"\"Initialize BPE tokenizer\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.token_id = 0\n",
    "        \n",
    "    def build_vocab(self, text):\n",
    "        \"\"\"Build initial vocabulary from characters\"\"\"\n",
    "        unique_chars = set(text)\n",
    "        self.vocab = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
    "        self.token_id = len(self.vocab)\n",
    "        print(f\"Initial vocab size: {len(self.vocab)}\")\n",
    "        return self.vocab\n",
    "    \n",
    "    def get_stats(self, split_text):\n",
    "        \"\"\"Find the most frequent pair of tokens\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word_tokens, freq in split_text.items():\n",
    "            for i in range(len(word_tokens) - 1):\n",
    "                pair = (word_tokens[i], word_tokens[i+1])\n",
    "                pairs[pair] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, split_text):\n",
    "        \"\"\"Merge a frequent pair in the split text\"\"\"\n",
    "        new_split = {}\n",
    "        # Faster regex-free merge\n",
    "        for word_tokens, freq in split_text.items():\n",
    "            new_word_tokens = []\n",
    "            i = 0\n",
    "            while i < len(word_tokens):\n",
    "                if i < len(word_tokens) - 1 and word_tokens[i] == pair[0] and word_tokens[i+1] == pair[1]:\n",
    "                    new_word_tokens.append(pair[0] + pair[1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word_tokens.append(word_tokens[i])\n",
    "                    i += 1\n",
    "            new_split[tuple(new_word_tokens)] = freq\n",
    "        return new_split\n",
    "\n",
    "    def train(self, text):\n",
    "        \"\"\"Train BPE tokenizer until vocab reaches vocab_size\"\"\"\n",
    "        self.build_vocab(text)\n",
    "        \n",
    "        # Initialize split text: dict of {tuple_of_tokens: frequency}\n",
    "        words = text.split()\n",
    "        word_counts = Counter(words)\n",
    "        split_text = {tuple(list(word)): count for word, count in word_counts.items()}\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        print(f\"Goal: {num_merges} merges to reach vocab size {self.vocab_size}\")\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(split_text)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_token = \"\".join(best_pair)\n",
    "            \n",
    "            self.vocab[new_token] = self.token_id\n",
    "            self.token_id += 1\n",
    "            self.merges[best_pair] = new_token\n",
    "            split_text = self.merge_vocab(best_pair, split_text)\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Iteration {i + 1}: Vocab size = {len(self.vocab)}, Merged {best_pair} -> {new_token}\")\n",
    "\n",
    "        print(f\"\\nBPE Training Complete!\")\n",
    "        print(f\"Final vocab size: {len(self.vocab)}\")\n",
    "        return self\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text into token IDs\"\"\"\n",
    "        # Simple greedy encoding for demonstration (BPE usually uses max-priority merges)\n",
    "        words = text.split()\n",
    "        encoded = []\n",
    "        # Build a list of merges in priority order\n",
    "        merge_list = list(self.merges.items())\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = list(word)\n",
    "            for pair, new_token in merge_list:\n",
    "                new_word_tokens = []\n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if i < len(word_tokens) - 1 and word_tokens[i] == pair[0] and word_tokens[i+1] == pair[1]:\n",
    "                        new_word_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                word_tokens = new_word_tokens\n",
    "            \n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    encoded.append(self.vocab[token])\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        tokens = [id_to_token.get(idx, \"\") for idx in token_ids]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save tokenizer to file\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({'vocab': self.vocab, 'merges': self.merges}, f)\n",
    "        print(f\"Tokenizer saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load tokenizer from file\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.vocab = data['vocab']\n",
    "            self.merges = data['merges']\n",
    "        print(f\"Tokenizer loaded from {filepath}\")\n",
    "        return self\n",
    "\n",
    "# Initialize and train BPE tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=250)\n",
    "tokenizer.train(training_text)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save('bpe_tokenizer.pkl')\n",
    "\n",
    "# Display sample vocab\n",
    "print(\"\\nSample vocabulary (first 20 and last 10 tokens):\")\n",
    "vocab_items = list(tokenizer.vocab.items())\n",
    "for token, idx in vocab_items[:20]:\n",
    "    print(f\"  {idx}: '{token}'\")\n",
    "print(\"  ...\")\n",
    "for token, idx in vocab_items[-10:]:\n",
    "    print(f\"  {idx}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32a727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
